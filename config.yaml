# Job specs define the configuration for Bacalhau jobs
jobs:
- id: metadata-job # This is the key used when looking for a job
  type: bacalhau # This job will run on the Bacalhau executor
  image: ghcr.io/bacalhau-project/amplify/tika:0.0.4
  entrypoint:
  - /usr/local/bin/run
- id: merge-job
  image: ubuntu
  entrypoint:
  - bash
  - -c
  - >-
    if [ -d /inputs ] ; then cp -r /inputs/* /outputs ; else cp /inputs /outputs/blob ; fi &&
    find / -iwholename '/inputs*metadata.json' | while read line ; do
    result=$(
    echo $line |
    sed 's/\(.*\)outputs\//\1/g' |
    sed 's/\/inputs[0-9]*//g' |
    sed -r 's/(.+)\//\1./g'
    ) ;
    output=/outputs${result} ;
    mkdir -p $(dirname ${output}) ;
    echo "Copying $line to $output" ;
    cp $line $output
    ; done
- id: tree-job
  image: ubuntu
  entrypoint:
  - bash
  - -c
  - >-
    ls -R /inputs
# A job that accepts a root input and passes it to the output -- the root of all jobs
- id: root-job
  type: internal # This job will run on an internal executor. Internal job, doesn't leave Amplify.
  internal_job_id: root-job # Link to internal job ID, must exist in the codebase
- id: image-resize-job # This job resizes images recursively
  type: bacalhau
  image: ghcr.io/bacalhau-project/amplify/magick:0.0.1
  entrypoint:
  - /usr/local/bin/run
- id: video-resize-job
  type: bacalhau
  image: ghcr.io/bacalhau-project/amplify/ffmpeg:0.0.2
  entrypoint:
  - /usr/local/bin/run
- id: csv-profiling-job
  type: bacalhau
  image: ghcr.io/bacalhau-project/amplify/ydata-profiling:0.0.3
  entrypoint:
  - /usr/local/bin/run
- id: csv-frictionless-job
  type: bacalhau
  image: ghcr.io/bacalhau-project/amplify/frictionless:0.0.2
- id: detection-job
  type: bacalhau
  image: ghcr.io/bacalhau-project/amplify/detection:0.0.3
  entrypoint:
  - /usr/local/bin/run

# Amplify Work Graph specification
# Each item in the list is a node in the execution graph. A single request 
# (typically a single CID) runs this whole graph. 
graph:
- id: root-node # This is the root of the dag, where the request CID is placed
  job_id: root-job
  inputs:
  - root: true # Identifies that this is a root node
    path: /inputs # Path where inputs will be placed
  outputs:
  - # id: default # Specify custom output id (default: "default")
    path: /outputs # Path where job places outputs
- id: metadata-node 
  job_id: metadata-job 
  inputs:
  - node_id: root-node
    # output_id: custom_id # Connect to custom output id (default: "default")
    path: /inputs
  outputs:
  - path: /outputs
- id: image-resize-node
  job_id: image-resize-job
  inputs:
  - node_id: metadata-node
    predicate: '.*image\/.*'
  - node_id: root-node
    path: /inputs/image/
  outputs:
  - path: /outputs
- id: video-resize-node
  job_id: video-resize-job
  inputs:
  - node_id: metadata-node
    predicate: '.*video\/.*'
  - node_id: root-node
    path: /inputs
  outputs:
  - path: /outputs
- id: csv-frictionless-node
  job_id: csv-frictionless-job
  inputs:
  - node_id: metadata-node
    predicate: '.*text\/csv.*'
  - node_id: root-node
    path: /inputs/
  outputs:
  - path: /outputs
- id: csv-profiling-node
  job_id: csv-profiling-job
  inputs:
  - node_id: csv-frictionless-node
    predicate: '.*"valid":true.*'
  - node_id: root-node
    path: /inputs/
- id: detection-node
  job_id: detection-job
  inputs:
  - node_id: metadata-node
    predicate: '.*(video\/|image\/).*'
  - node_id: root-node
    path: /inputs
  outputs:
  - path: /outputs
- id: merge-node
  job_id: merge-job
  inputs:
  - node_id: root-node
    path: /inputs/src
  - node_id: metadata-node 
    path: /inputs/metadata
  - node_id: video-resize-node
    path: /inputs/videos
  - node_id: detection-node
    path: /inputs/detections
  - node_id: image-resize-node
    path: /inputs/images
  - node_id: csv-frictionless-node
    path: /inputs/csv-frictionless
  - node_id: csv-profiling-node
    path: /inputs/csv-profile
  outputs:
  - path: /outputs
